{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96002503-9442-4842-9a58-8fc19a770fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models\n",
    "from torchvision.datasets import ImageFolder\n",
    "! pip install torchsummary\n",
    "import torchsummary\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torchvision.transforms import v2\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix , accuracy_score, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa104823-024f-4ce2-8120-97d8af71b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\"path\":[],\"label\":[], \"class_id\":[]})\n",
    "train_path = './PALM/Train/PALM-Training400'\n",
    "folder_list = ['./PALM/Train/PALM-Training400/'+ item for item in os.listdir(train_path)]\n",
    "label_dict = {\n",
    "    \"H\":0, \n",
    "    \"N\":0,\n",
    "    \"P\":1,\n",
    "}\n",
    "for i in range(len(folder_list)):\n",
    "    \n",
    "        folder = folder_list[i].split('/')[-1][0]\n",
    "        if folder == 'H':\n",
    "            folder = 'N'\n",
    "        new_data =pd.DataFrame({\"path\":folder_list[i],\"label\":folder, \"class_id\":label_dict[folder]}, index=[1])\n",
    "        train_df = pd.concat([train_df, new_data], ignore_index=True)\n",
    "\n",
    "train_df[[\"path\"]] = train_df[[\"path\"]].astype(str)\n",
    "train_df[[\"label\"]] = train_df[[\"label\"]].astype(str)\n",
    "train_df[[\"class_id\"]] = train_df[[\"class_id\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3417a-4687-4fa8-9469-5b134a209d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame({\"path\":[],\"label\":[], \"class_id\":[]})\n",
    "val_path = r'./PALM/Validation/Validation-400'\n",
    "folder_list = [r'./PALM/Validation/Validation-400/'+ item for item in os.listdir(val_path)]\n",
    "val_labels = pd.read_excel(r'./PALM/Validation/PM_Label_and_Fovea_Location.xlsx')\n",
    "val_labels_list = val_labels['Label'].values.tolist()\n",
    "val_labels_name = {\n",
    "    1:'P',\n",
    "    0:'N'\n",
    "}\n",
    "for i in range(len(folder_list)):\n",
    "        folder = folder_list[i].split('/')[-1][0]\n",
    "        new_data =pd.DataFrame({\"path\":folder_list[i],\"label\":val_labels_name[val_labels_list[i]], \"class_id\":val_labels_list[i]}, index=[1])\n",
    "        val_df = pd.concat([val_df, new_data], ignore_index=True)\n",
    "\n",
    "val_df[[\"path\"]] = val_df[[\"path\"]].astype(str)\n",
    "val_df[[\"label\"]] = val_df[[\"label\"]].astype(str)\n",
    "val_df[[\"class_id\"]] = val_df[[\"class_id\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f04844-aebd-48d8-8790-77c185a1a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\"path\":[],\"label\":[], \"class_id\":[]})\n",
    "test_path = r'./PALM/Test/PALM-Testing400-Images'\n",
    "folder_list = [r'./PALM/Test/PALM-Testing400-Images/'+ item for item in os.listdir(test_path)]\n",
    "test_labels = pd.read_excel(r'./PALM/Test/PM_Label_and_Fovea_Location.xlsx')\n",
    "test_labels_list = test_labels['Label'].values.tolist()\n",
    "test_labels_name = {\n",
    "    1:'P',\n",
    "    0:'N'\n",
    "}\n",
    "for i in range(len(folder_list)):\n",
    "        folder = folder_list[i].split('/')[-1][0]\n",
    "        new_data =pd.DataFrame({\"path\":folder_list[i],\"label\":test_labels_name[test_labels_list[i]], \"class_id\":test_labels_list[i]}, index=[1])\n",
    "        test_df = pd.concat([test_df, new_data], ignore_index=True)\n",
    "\n",
    "test_df[[\"path\"]] = test_df[[\"path\"]].astype(str)\n",
    "test_df[[\"label\"]] = test_df[[\"label\"]].astype(str)\n",
    "test_df[[\"class_id\"]] = test_df[[\"class_id\"]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825b734-fab0-4ebd-926a-88ef7839c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_imgs = 15\n",
    "idx = np.random.randint(0, len(train_df),size=show_imgs)\n",
    "fig, axes = plt.subplots(show_imgs//5, 5, figsize=(15,10))\n",
    "axes = axes.flatten()\n",
    "for i, ax in enumerate(axes):\n",
    "    full_path = train_df.loc[idx[i]]['path']\n",
    "    ax.imshow(plt.imread(full_path))\n",
    "    ax.set_title(train_df.loc[idx[i]]['label'])\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c285679-123c-4d85-851c-bfac40725c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_imgs = 15\n",
    "idx = np.random.randint(0, len(test_df),size=show_imgs)\n",
    "fig, axes = plt.subplots(show_imgs//5, 5, figsize=(15,10))\n",
    "axes = axes.flatten()\n",
    "for i, ax in enumerate(axes):\n",
    "    full_path = test_df.loc[idx[i]]['path']\n",
    "    ax.imshow(plt.imread(full_path))\n",
    "    ax.set_title(test_df.loc[idx[i]]['label'])\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948607d2-1e32-4adb-8b48-000875a86785",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = v2.Compose([\n",
    "    v2.Resize(256),\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    v2.RandomErasing(p=0.5, scale=(0.1,0.15)),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "])\n",
    "\n",
    "val_transforms = v2.Compose([\n",
    "    v2.Resize((224,224)),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = v2.Compose([\n",
    "    v2.Resize((224,224)),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfbdb15-8ae1-4804-b256-733e8f254961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, transforms_):\n",
    "        self.df = dataframe\n",
    "        self.transforms_ = transforms_\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.df.iloc[index]['path']\n",
    "        # img = Image.open(image_path).convert(\"LA\")\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        # img = Image.open(image_path)\n",
    "        transformed_img = self.transforms_(img)\n",
    "        class_id = self.df.iloc[index]['class_id'].tolist()\n",
    "        return transformed_img, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125a249-e484-46da-9e09-364a65ae4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "# device = 'cpu'\n",
    "train_dataset = MyDataset(train_df, train_transforms)\n",
    "val_dataset = MyDataset(val_df, val_transforms)\n",
    "test_dataset = MyDataset(test_df, test_transforms)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01cc5e0-743a-4a16-b68b-f86511317191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_weights = F.softmax(self.W(x), dim=1)\n",
    "        output = attn_weights * x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5c81c-3e38-4f6a-8110-e13e5fefa177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size = 1\n",
    "model = models.efficientnet_v2_l(weights='DEFAULT')\n",
    "model.classifier[1] = nn.Sequential(\n",
    "    Attention(1280),\n",
    "    nn.Linear(1280, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model(torch.randn((16,3,256,256))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc598455-b4f8-450f-aed5-9fb62774dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def comp_result(y_test, y_pred, y_proba):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    \n",
    "    return accuracy,f1,auc,mcc,recall,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9ff15-8acc-43e6-b267-a0b618bbc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, lr_scheduler):\n",
    "    size = len(dataloader.dataset) # number of samples\n",
    "    num_batches = len(dataloader) # batches per epoch\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0 \n",
    "    for (data_,target_) in dataloader:\n",
    "        target_ = target_.type(torch.float32)\n",
    "#         print(target_.shape)\n",
    "#         target_ = torch.tensor(target_.reshape(-1,1))\n",
    "#         print(target_)\n",
    "        data_, target_ = data_.to(device), target_.to(device)\n",
    "        target_ = target_.reshape(-1,1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(data_)\n",
    "        loss = loss_fn(outputs,target_)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss = epoch_loss + loss.item()\n",
    "        \n",
    "        predictions = (outputs > 0.5).float() \n",
    "        correct_predictions += (predictions.to('cpu') == target_.to('cpu')).sum().item()\n",
    "    lr_scheduler.step()\n",
    "    return correct_predictions/size, epoch_loss/num_batches\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset) \n",
    "    num_batches = len(dataloader) \n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0 \n",
    "    predict_labels_com = []\n",
    "    test_labels_com = []\n",
    "    test_outputs_com = []\n",
    "    with torch.no_grad():\n",
    "        # This will disable backward propagation\n",
    "        model.eval()\n",
    "        for (data_,target_) in dataloader:\n",
    "            target_ = target_.type(torch.float32)\n",
    "            \n",
    "            data_, target_ = data_.to(device), target_.to(device)\n",
    "            target_ = target_.reshape(-1,1)\n",
    "            # Forward propagation\n",
    "            outputs = model(data_)\n",
    "            \n",
    "            # Computing loss \n",
    "            loss = loss_fn(outputs,target_)\n",
    "            # Computing statistics.\n",
    "            epoch_loss = epoch_loss + loss.item()\n",
    "            predictions = (outputs > 0.5).float() \n",
    "            correct_predictions += (predictions.to('cpu') == target_.to('cpu')).sum().item()\n",
    "            \n",
    "            predict_labels_com.extend(predictions.tolist())\n",
    "            test_labels_com.extend(target_.tolist())\n",
    "            test_outputs_com.extend(outputs.tolist())\n",
    "            \n",
    "        predict_labels_com = np.array(predict_labels_com)\n",
    "        test_labels_com = np.array(test_labels_com)\n",
    "        test_outputs_com = np.array(test_outputs_com)\n",
    "        accuracy,f1,auc,mcc,recall,precision = comp_result(test_labels_com,predict_labels_com,test_outputs_com)\n",
    "            \n",
    "    return  accuracy,f1,auc,mcc,recall,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e5150-7e95-4086-808c-befa61abf85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model.to(device)\n",
    "EPOCHS = 50\n",
    "\n",
    "logs = {\n",
    "    'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []\n",
    "}\n",
    "\n",
    "criterion  = nn.BCELoss()\n",
    "\n",
    "\n",
    "# Optimizer which will use gradients to train model.\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "weight_decay = 0.1\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=weight_decay, amsgrad=False)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)\n",
    "lr_milestones = [7, 14, 21, 28, 35]\n",
    "multi_step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=lr_milestones, gamma=0.1)\n",
    "\n",
    "# Earlystopping\n",
    "patience = 5\n",
    "counter = 0\n",
    "best_acc = -1\n",
    "best_auc = -1\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    train_acc, train_loss,  = train(train_loader, model, criterion, optimizer, multi_step_lr_scheduler)\n",
    "    print(\"val===================\",epoch)\n",
    "    val_accuracy,val_f1,val_auc,val_mcc,val_recall,val_precision = test(val_loader, model, criterion)\n",
    "    print('=======================')\n",
    "    print(f'EPOCH: {epoch} \\\n",
    "    train_loss: {train_loss:.4f}, train_acc: {train_acc:.3f} \\\n",
    "    val_loss: {val_loss:.4f}, val_acc: {val_acc:.3f} \\\n",
    "    Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n",
    "    \n",
    "    logs['train_loss'].append(train_loss)\n",
    "    logs['train_acc'].append(train_acc)\n",
    "    logs['val_loss'].append(val_auc)\n",
    "    logs['val_acc'].append(val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
